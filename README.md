To help analyze the performance of the pipeline and the underlying data model, we built a test environment on a 3-node MongoDB Atlas M20 cluster. This environment contained 1 million profile documents, 2.7 million coverage documents, and over 5 million mapping documents linking profiles to devices. 

The performance of the pipeline was measured using an application written in Go using the native MongoDB Go driver. The application ran queries in a total of 15 concurrent goroutines (for our purposes, we can think of a goroutine as a thread), split evenly across three connections to the database - one connection to each of the three nodes in the MongoDB replica set. This configuration allowed secondary nodes in the replica set to share the search load with the primary node and is a common practice in read intensive workloads. 

The application was run on an AWS t2.xlarge x86-64 server running Amazon Linux. Both it and the MongoDB Atlas cluster were deployed in the US-West-1 region. Performance was measured end to end, i.e. it included overheads in sending the query from the application to the database, and then returning the results from the database to the application. Each modification to the data model and pipeline design was run 300 times, each run using a random combination of city and device name. By not simply repeating the same combination of city and device name we avoided potentially masking issues with MongoDB being under-provisioned with enough memory for effective caching. The mean execution time of each of the 300 iterations was recorded. 